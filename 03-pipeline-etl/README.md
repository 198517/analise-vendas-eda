# ğŸ”„ Pipeline ETL com Python

## ğŸ¯ Objetivo

Implementar um pipeline completo de ETL (Extract, Transform, Load) para processar dados de vendas de mÃºltiplas fontes e carregar em um data warehouse.

## ğŸš€ Tecnologias Utilizadas

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white)
![SQLAlchemy](https://img.shields.io/badge/SQLAlchemy-D71F00?style=for-the-badge&logo=python&logoColor=white)

## âœ¨ Funcionalidades

### ğŸ“¥ Extract (ExtraÃ§Ã£o)
- Leitura de arquivos CSV, Excel e JSON
- ConexÃ£o com APIs REST
- ExtraÃ§Ã£o de bancos de dados SQL
- ValidaÃ§Ã£o de dados na origem

### ğŸ”§ Transform (TransformaÃ§Ã£o)
- Limpeza de dados (valores nulos, duplicatas)
- PadronizaÃ§Ã£o de formatos
- Enriquecimento de dados
- CÃ¡lculo de mÃ©tricas agregadas
- ValidaÃ§Ã£o de qualidade de dados

### ğŸ“¤ Load (Carga)
- Carga incremental em PostgreSQL
- Upsert (insert ou update)
- Logging de operaÃ§Ãµes
- Tratamento de erros

## ğŸ“ Estrutura do Projeto

```
pipeline-etl/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ database.yaml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â”œâ”€â”€ csv_extractor.py
â”‚   â”‚   â”œâ”€â”€ api_extractor.py
â”‚   â”‚   â””â”€â”€ db_extractor.py
â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â”œâ”€â”€ data_cleaner.py
â”‚   â”‚   â”œâ”€â”€ data_validator.py
â”‚   â”‚   â””â”€â”€ data_enricher.py
â”‚   â”œâ”€â”€ load/
â”‚   â”‚   â””â”€â”€ db_loader.py
â”‚   â””â”€â”€ pipeline.py
â”œâ”€â”€ logs/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ archive/
â”œâ”€â”€ tests/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ”„ Fluxo do Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EXTRACT   â”‚ --> â”‚  TRANSFORM   â”‚ --> â”‚    LOAD     â”‚
â”‚             â”‚     â”‚              â”‚     â”‚             â”‚
â”‚ â€¢ CSV       â”‚     â”‚ â€¢ Limpeza    â”‚     â”‚ â€¢ PostgreSQLâ”‚
â”‚ â€¢ API       â”‚     â”‚ â€¢ ValidaÃ§Ã£o  â”‚     â”‚ â€¢ Upsert    â”‚
â”‚ â€¢ Database  â”‚     â”‚ â€¢ AgregaÃ§Ã£o  â”‚     â”‚ â€¢ Logging   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Como Executar

### PrÃ©-requisitos
```bash
Python 3.8+
PostgreSQL 12+
```

### ConfiguraÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/198517/pipeline-etl.git
cd pipeline-etl
```

2. Instale as dependÃªncias:
```bash
pip install -r requirements.txt
```

3. Configure o banco de dados em `config/database.yaml`:
```yaml
database:
  host: localhost
  port: 5432
  database: vendas_dw
  user: seu_usuario
  password: sua_senha
```

4. Execute o pipeline:
```bash
python src/pipeline.py
```

## ğŸ“Š Exemplo de Uso

```python
from src.pipeline import ETLPipeline

# Inicializa o pipeline
pipeline = ETLPipeline(config_path='config/database.yaml')

# Executa o pipeline completo
pipeline.run(
    extract_sources=['csv', 'api'],
    transform_rules=['clean', 'validate', 'enrich'],
    load_mode='incremental'
)

# Verifica logs
pipeline.get_execution_log()
```

## ğŸ“¦ DependÃªncias

```
pandas==2.0.0
sqlalchemy==2.0.0
psycopg2-binary==2.9.0
pyyaml==6.0
requests==2.31.0
python-dotenv==1.0.0
```

## ğŸ“ Aprendizados

Este projeto demonstra:
- âœ… Arquitetura de pipelines ETL
- âœ… IntegraÃ§Ã£o com mÃºltiplas fontes de dados
- âœ… TransformaÃ§Ã£o e limpeza de dados
- âœ… Carga em data warehouse
- âœ… Tratamento de erros e logging
- âœ… Boas prÃ¡ticas de engenharia de dados

## ğŸ” ValidaÃ§Ãµes Implementadas

- VerificaÃ§Ã£o de tipos de dados
- ValidaÃ§Ã£o de valores nulos
- DetecÃ§Ã£o de duplicatas
- VerificaÃ§Ã£o de integridade referencial
- ValidaÃ§Ã£o de regras de negÃ³cio

## ğŸ“ PrÃ³ximos Passos

- [ ] Implementar processamento paralelo
- [ ] Adicionar orquestraÃ§Ã£o com Apache Airflow
- [ ] Implementar data quality checks
- [ ] Adicionar monitoramento com Prometheus
- [ ] Criar testes unitÃ¡rios e de integraÃ§Ã£o

## ğŸ‘¤ Autor

**Anderson de Lima**
- LinkedIn: [anderson-de-lima-analista-de-dados](https://www.linkedin.com/in/anderson-de-lima-analista-de-dados/)
- GitHub: [@198517](https://github.com/198517)

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT.

---

â­ Se este projeto foi Ãºtil para vocÃª, considere dar uma estrela!
